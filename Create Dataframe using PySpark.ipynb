{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe194b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e81c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c185d888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add2fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7229ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "    or a :class:`numpy.ndarray`.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    \n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "    \n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "        later.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring. The first few rows will be used\n",
      "        if ``samplingRatio`` is ``None``.\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "        .. versionadded:: 2.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create a DataFrame from a list of tuples.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)]).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    Create a DataFrame from a list of dictionaries\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    Create a DataFrame from an RDD.\n",
      "    \n",
      "    >>> rdd = spark.sparkContext.parallelize([('Alice', 1)])\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    Create a DataFrame from Row instances.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    Create a DataFrame with the explicit schema specified.\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    Create a DataFrame from a pandas DataFrame.\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    Create  a DataFrame from an RDD with the schema in DDL formatted string.\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    \n",
      "    When the type is unmatched, it throws an exception.\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b185a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
